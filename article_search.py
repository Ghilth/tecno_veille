import http.client
import json
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import os
from transformers import pipeline




load_dotenv()  # Charge les variables depuis .env
api_key = os.getenv("api_key")  # R√©cup√®re la cl√© API


def summarize(lien):
    # √âtape 1: R√©cup√©rer le contenu de la page
    try:
        response = requests.get(lien)
        response.raise_for_status()  # V√©rifie si la requ√™te a r√©ussi
        page_content = response.text
    except requests.exceptions.RequestException as e:
        return f"Erreur lors de la r√©cup√©ration de la page : {e}"
    

    # √âtape 2: Extraire le texte de l'article
    soup = BeautifulSoup(page_content, "html.parser")

    
    # En fonction de la structure du site, tu devras peut-√™tre ajuster ceci
    paragraphs = soup.find_all('p')  # Extraire tous les paragraphes <p>
    
    # Joindre les paragraphes pour former le texte complet de l'article
    article_text = " ".join([para.get_text() for para in paragraphs])
    
    # √âtape 3: Utiliser un mod√®le pour g√©n√©rer le r√©sum√©
    summarizer = pipeline("summarization")  # Pipeline de r√©sum√© de Hugging Face
    
    # Diviser le texte en morceaux si n√©cessaire (limite de longueur pour le mod√®le)
    max_input_length = 2000  # La limite de tokens pour de nombreux mod√®les GPT-2 et GPT-3
    
    if len(article_text) > max_input_length:
        article_text = article_text[:max_input_length]  # Prendre une portion du texte

    # R√©sumer l'article
    summary = summarizer(article_text, max_length=150, min_length=50, do_sample=False)
    
    return summary[0]['summary_text']




def search_articles(mots_cles, annee_debut=None, annee_fin=None):
    if not api_key:
        raise ValueError("La cl√© API est manquante !")

    print("üîë Cl√© API utilis√©e :", api_key)  # Debugging

    # Connexion √† l'API
    conn = http.client.HTTPSConnection("google.serper.dev")
    
    # Pr√©parer le payload correctement
    payload = json.dumps({"q": " ".join(mots_cles)})
    
    headers = {
        'X-API-KEY': api_key,
        'Content-Type': 'application/json'
    }
    
    # Envoyer la requ√™te POST
    conn.request("POST", "/scholar", payload, headers)
    res = conn.getresponse()
    
    # V√©rification du statut de la r√©ponse
    if res.status != 200:
        print(f"‚ùå Erreur API ({res.status}):", res.read().decode("utf-8"))
        return []

    data = res.read()
    print("üìú R√©ponse brute de l'API:", data[:500])  # Afficher seulement une partie pour √©viter trop d'affichage
    
    try:
        articles = json.loads(data.decode("utf-8"))
    except json.JSONDecodeError:
        print("‚ùå Erreur de d√©codage JSON")
        return []

    # Liste des r√©sultats
    resultats = []
    
    # Parcourir les r√©sultats
    for article in articles.get("organic", []):
        annee_article = article.get("year", None)
        
        # Appliquer le filtre de date
        if annee_debut and annee_article and int(annee_article) < annee_debut:
            continue
        if annee_fin and annee_article and int(annee_article) > annee_fin:
            continue
        
        # Extraire les informations utiles
        info_article = {
            "Titre": article.get("title", ""),
            "Lien": article.get("link", ""),
            "Extrait": article.get("snippet", ""),
            "Auteur": article.get("author", ""),
            "Ann√©e": annee_article,
            "Citations": article.get("citedBy", ""),
        }
        resultats.append(info_article)
    
    return resultats

# Exemple d'utilisation
mots_cles = ["model deployment", "deep learning"]
annee_debut = 2018
annee_fin = 2024

resultats = search_articles(mots_cles, annee_debut, annee_fin)
print(resultats)