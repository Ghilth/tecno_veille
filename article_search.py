import http.client
import json
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import os
from transformers import pipeline




load_dotenv()  # Charge les variables depuis .env
api_key = os.getenv("api_key")  # R√©cup√®re la cl√© API


def summarize(lien):
    # √âtape 1: R√©cup√©rer le contenu de la page
    try:
        response = requests.get(lien)
        response.raise_for_status()  # V√©rifie si la requ√™te a r√©ussi
        page_content = response.text
    except requests.exceptions.RequestException as e:
        return f"Erreur lors de la r√©cup√©ration de la page : {e}"
    

    # √âtape 2: Extraire le texte de l'article
    soup = BeautifulSoup(page_content, "html.parser")

    
    # En fonction de la structure du site, tu devras peut-√™tre ajuster ceci
    paragraphs = soup.find_all('p')  # Extraire tous les paragraphes <p>
    
    # Joindre les paragraphes pour former le texte complet de l'article
    article_text = " ".join([para.get_text() for para in paragraphs])
    
    # √âtape 3: Utiliser un mod√®le pour g√©n√©rer le r√©sum√©
    summarizer = pipeline("summarization")  # Pipeline de r√©sum√© de Hugging Face
    
    # Diviser le texte en morceaux si n√©cessaire (limite de longueur pour le mod√®le)
    max_input_length = 2000  # La limite de tokens pour de nombreux mod√®les GPT-2 et GPT-3
    
    if len(article_text) > max_input_length:
        article_text = article_text[:max_input_length]  # Prendre une portion du texte

    # R√©sumer l'article
    summary = summarizer(article_text, max_length=150, min_length=50, do_sample=False)
    
    return summary[0]['summary_text']




def search_articles(mots_cles, annee_debut=None, annee_fin=None):

    print("Cl√© API utilis√©e :", api_key)  # V√©rification

    # Connexion √† l'API
    conn = http.client.HTTPSConnection("google.serper.dev")
    
    # Pr√©parer le payload (les mots-cl√©s sous forme de requ√™tes)
    payload = json.dumps([{"q": mot} for mot in mots_cles])
    
    headers = {
        'X-API-KEY': api_key,  # Remplace avec ta propre cl√© API
        'Content-Type': 'application/json'
    }
    
    # Envoyer la requ√™te POST
    conn.request("POST", "/scholar", payload, headers)
    res = conn.getresponse()
    data = res.read()

    print("R√©ponse brute de l'API:", data)  # üîç Debug
    
    # Convertir la r√©ponse en format JSON
    articles = json.loads(data.decode("utf-8"))
    
    # Liste pour stocker les r√©sultats
    resultats = []
    
    # Parcourir les r√©sultats de chaque requ√™te (mots-cl√©s)
    for resultat in articles:
        for article in resultat.get("organic", []):
            # R√©cup√©rer l'ann√©e de l'article
            annee_article = article.get("year", None)
            
            # Si une plage d'ann√©es est d√©finie, on filtre les articles
            if annee_debut and annee_article and int(annee_article) < annee_debut:
                continue
            if annee_fin and annee_article and int(annee_article) > annee_fin:
                continue
            
            # Extraire les informations pertinentes
            info_article = {
                "Titre": article.get("title", ""),
                "Lien": article.get("link", ""),
                "Extrait": article.get("snippet", ""),
                #"Resume": summarize(article.get("link", "")),
                "Auteur": article.get("author", ""),
                "Ann√©e": annee_article,
                "Citations": article.get("citedBy", "")
            }
            resultats.append(info_article)
    
    return resultats


# Exemple d'utilisation avec tranche d'ann√©es
#mots_cles = ["model depoyment", "deep learning"]
#annee_debut = 2018
#annee_fin = 2024

#print(search_articles(mots_cles,annee_debut,annee_fin))
#print(summarize("https://kinsta.com/fr/base-de-connaissances/site-statique/")) 

